{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e387428",
   "metadata": {},
   "source": [
    "# 1. GPT 모델\n",
    "\n",
    "기존의 encoder-decoder 구조인 Transformer 모델을 변형하여 GPT 모델 구조를 만들기 위해서는 아래와 같은 변경이 필요합니다. 이때 빨간색 박스는 사라져야할 컴포넌트이며, 초록색 박스는 변경되어야 하는 컴포넌트를 의미합니다.\n",
    "![](https://github.com/minkj1992/ai/blob/main/static/Untitled-2024-06-21-1948.png?raw=true)\n",
    "\n",
    "openai에서 발표한 [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) 논문에 따르면, decoder 모델을 토대로 generalized한 pre-trained 모델을 만듭니다. 그러므로 transformer의 사라져야할 부분은 아래와 같습니다.\n",
    "\n",
    "#### 사라져야할 부분\n",
    "> 빨간 네모\n",
    "1. `Encoder`\n",
    "2. `encoder-decoder attention layer`\n",
    "\n",
    "\n",
    "위 논문에 따르면 변경되어야할 부분은 아래와 같습니다.\n",
    "\n",
    "#### 변경되어야 할 부분\n",
    "> 초록 네모\n",
    "\n",
    "1. Positional encoding -> Positional embedding\n",
    "2. Target input -> `Task-specific input transformations`\n",
    "\n",
    "해당 논문에서는 또한 기존의 sin,cos을 활용한 positional 계산 방식을 positional embedding (max_sentence_len, d_model)을 생성하여 학습되도록 합니다. \n",
    "\n",
    "Target input 또한 달라져야 하는데, 이번 시간에는 pretrain + fine tunning 총 2번 학습했던 논문과 달리 1번만 학습할 예정이기 때문에, input을 변형해서 넣어주도록 하였습니다. \n",
    "\n",
    "현재 데이터 셋은 사용자의 질문인, df['Q']와 챗봇의 답변인 df['A']가 존재하며, 이 두 데이터 모두 챗봇에 필요하며 부족한 데이터를 고려하여, 합쳐서 학습해주도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1dd2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb6d2a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10819</th>\n",
       "      <td>오랫동안 썸만 타. 뭐가 문제임?</td>\n",
       "      <td>고백을 안 한 게 문제일 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659</th>\n",
       "      <td>친구랑 싸웠어</td>\n",
       "      <td>싸우면서 정 들 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6939</th>\n",
       "      <td>어떻게 살아가야 할까</td>\n",
       "      <td>태연하게 시작해보세요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>기분이 이상해</td>\n",
       "      <td>무슨 이유인지 생각해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>고기 먹고 싶어</td>\n",
       "      <td>저기압에는 고기앞이죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>어이없어</td>\n",
       "      <td>그냥 잊어버리세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>바다 가자고 하면 갈까?</td>\n",
       "      <td>같이 가자고 말해보세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>온전히 내 편이길 바라는 누군가가 옆에 있었으면 좋겠는</td>\n",
       "      <td>제가 당신 곁에 있어드릴게요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>내여잘 건드린놈들</td>\n",
       "      <td>벌 받을 거예요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9591</th>\n",
       "      <td>다시 웃게 해준 사람</td>\n",
       "      <td>그 사람이 당신의 홈닥터군요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9580</th>\n",
       "      <td>눈을 감아도 보고 싶어</td>\n",
       "      <td>상사병 걸리기전에 찾아가보세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7473</th>\n",
       "      <td>이별 후 한달째</td>\n",
       "      <td>아직 많이 힘드시겠군요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>버릴 건 버려야 겠지</td>\n",
       "      <td>잘 버리는 것도 중요해요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10820</th>\n",
       "      <td>오랫동안 썸만 타고 솔로인건 왜그래?</td>\n",
       "      <td>썸이 아니었을지도 몰라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>스케일링 하기 귀찮아</td>\n",
       "      <td>주기적으로 해주면 좋대요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Q                     A  label\n",
       "10819              오랫동안 썸만 타. 뭐가 문제임?  고백을 안 한 게 문제일 거 같아요.      2\n",
       "4659                          친구랑 싸웠어         싸우면서 정 들 거예요.      0\n",
       "6939                      어떻게 살아가야 할까          태연하게 시작해보세요.      1\n",
       "376                           기분이 이상해       무슨 이유인지 생각해보세요.      0\n",
       "181                          고기 먹고 싶어          저기압에는 고기앞이죠.      0\n",
       "3058                             어이없어            그냥 잊어버리세요.      0\n",
       "1891                    바다 가자고 하면 갈까?          같이 가자고 말해보세요      0\n",
       "7213   온전히 내 편이길 바라는 누군가가 옆에 있었으면 좋겠는      제가 당신 곁에 있어드릴게요.      1\n",
       "5893                        내여잘 건드린놈들             벌 받을 거예요.      1\n",
       "9591                      다시 웃게 해준 사람      그 사람이 당신의 홈닥터군요.      2\n",
       "9580                     눈을 감아도 보고 싶어     상사병 걸리기전에 찾아가보세요.      2\n",
       "7473                         이별 후 한달째         아직 많이 힘드시겠군요.      1\n",
       "2028                      버릴 건 버려야 겠지        잘 버리는 것도 중요해요.      0\n",
       "10820            오랫동안 썸만 타고 솔로인건 왜그래?        썸이 아니었을지도 몰라요.      2\n",
       "2655                      스케일링 하기 귀찮아        주기적으로 해주면 좋대요.      0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getenv('HOME')+'/aiffel/transformer_chatbot/data/ChatbotData .csv' \n",
    "origin = pd.read_csv(path) \n",
    "\n",
    "print(origin.shape)\n",
    "origin.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15b49c",
   "metadata": {},
   "source": [
    "# 1. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd314645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23646, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>감정조절이 안되서 문자 보냈네</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10735</th>\n",
       "      <td>연애보다 썸이 좋아.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13404</th>\n",
       "      <td>서로 마음만 맞으면 가능해요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12714</th>\n",
       "      <td>공부한 만큼 나올 거예요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>스키 강습 받아야 될까?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12926</th>\n",
       "      <td>꿈같은 이야기네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>식욕이 없어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>너한테 쓰는 편지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8412</th>\n",
       "      <td>하루하루가 지옥같아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6758</th>\n",
       "      <td>슬픈 예감대로 되어가는 현실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>마음이 울적해</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23074</th>\n",
       "      <td>성격도 계속 바뀌니 걱정 말아요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6109</th>\n",
       "      <td>두달 반</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>너 또 뭐할 줄 알아?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>오늘 또 늦잠잤어</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text\n",
       "5448     감정조절이 안되서 문자 보냈네\n",
       "10735         연애보다 썸이 좋아.\n",
       "13404    서로 마음만 맞으면 가능해요.\n",
       "12714      공부한 만큼 나올 거예요.\n",
       "2658        스키 강습 받아야 될까?\n",
       "12926          꿈같은 이야기네요.\n",
       "2763               식욕이 없어\n",
       "5973            너한테 쓰는 편지\n",
       "8412           하루하루가 지옥같아\n",
       "6758      슬픈 예감대로 되어가는 현실\n",
       "1481              마음이 울적해\n",
       "23074  성격도 계속 바뀌니 걱정 말아요.\n",
       "6109                 두달 반\n",
       "929          너 또 뭐할 줄 알아?\n",
       "3286            오늘 또 늦잠잤어"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = origin.copy()\n",
    "\n",
    "df_q = pd.DataFrame({\n",
    "    'text': tmp['Q']\n",
    "})\n",
    "df_a = pd.DataFrame({\n",
    "    'text': tmp['A']\n",
    "})\n",
    "\n",
    "df = pd.concat([df_q, df_a], ignore_index=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5ecac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23646, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e43e105f3a42e78e7b163794f44448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19436, 1)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def postprocess(df):\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.drop_duplicates(subset = ['text'], inplace = True)\n",
    "df['text'] = df['text'].progress_apply(preprocess)\n",
    "postprocess(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88629e27",
   "metadata": {},
   "source": [
    "## 2. Data prepare\n",
    "\n",
    "- 질문,답변 각자 적절한 maximum 크기만 남기고 지운다.\n",
    "- `[SOS] 질문 [DELIM] 답변 [EOS]` 합친다 -> teacher forcing 학습\n",
    "- 추론단계에서는 사용자 질문 -> 챗봇 답변에서 [DELIM] ~ [EOS]까지만 return.\n",
    "\n",
    "\n",
    "## (WIP) 추후 도입 방법\n",
    "1. Genralized Pretrain\n",
    "2. Fine tunning\n",
    "\n",
    "두가지를 수행하기 위해서 2가지에 필요한 데이터를 준비하겠습니다.\n",
    "\n",
    "1번을 위해서는 하나의 row에 속한 df['Q']와 df['A']를 2개의 row로 나눠서 df['text']로 만들어 0...i -> i+1예측 teacher forcing을 합니다.\n",
    "\n",
    "이후 2번은 `[SOS] Q [DELIM] A_set [EOS]`형태의 문장을 A_set만큼 만들어, 정답인 A를 예측하도록 하는 softmax를 생성, 이를 위해서는 정답이 아닌 negative sampling이 필요해서, 다음 기회에 시도\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f912ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text의 최소 길이 : 1\n",
      "text의 최대 길이 : 24\n",
      "text의 평균 길이 : 4.315651368594361\n",
      "전체 샘플 중 길이가 9 이하인 샘플의 비율: 0.9842045688413253\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if len(s.split()) <= max_len:\n",
    "            cnt = cnt + 1\n",
    "    print(\n",
    "        \"전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s\"\n",
    "        % (max_len, (cnt / len(nested_list)))\n",
    "    )\n",
    "\n",
    "    \n",
    "len_text = [len(s.split()) for s in df['text']]\n",
    "max_len_text = 9\n",
    "print(\"text의 최소 길이 : {}\".format(np.min(len_text)))\n",
    "print(\"text의 최대 길이 : {}\".format(np.max(len_text)))\n",
    "print(\"text의 평균 길이 : {}\".format(np.mean(len_text)))\n",
    "below_threshold_len(max_len_text, df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b942357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 19129\n",
      "삭제된 샘플수: 307\n"
     ]
    }
   ],
   "source": [
    "before = len(df)\n",
    "def is_within(text, max_len):\n",
    "    return len(text.split()) <= max_len\n",
    "_filter = df[\"text\"].apply(is_within, max_len=max_len_text)\n",
    "df = df[_filter]\n",
    "print(\"전체 샘플수 :\", (len(df)))\n",
    "print(f\"삭제된 샘플수: {before - len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c5f562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS 번호 : [8817]\n",
      "EOS 번호 : [8818]\n",
      "8819\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    df[\"text\"],\n",
    "    target_vocab_size=2**13,\n",
    ")\n",
    "SOS, EOS = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"SOS 번호 :\", [tokenizer.vocab_size])\n",
    "print(\"EOS 번호 :\", [tokenizer.vocab_size + 1])\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "745ae419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 19129\n"
     ]
    }
   ],
   "source": [
    "def tokenize(texts):\n",
    "    tokenized = []\n",
    "    max_len = 0\n",
    "    for text in texts:\n",
    "        tokenized_txt = SOS + tokenizer.encode(text) + EOS\n",
    "        max_len = max(max_len, len(tokenized_txt))\n",
    "        tokenized.append(tokenized_txt)\n",
    "\n",
    "    # max_length 으로 모든 데이터셋을 패딩\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized, maxlen=max_len, padding=\"post\"\n",
    "    ), max_len\n",
    "\n",
    "\n",
    "texts, MAX_LENGTH = tokenize(df[\"text\"])\n",
    "print(MAX_LENGTH, len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1fc23bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19129"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].shape\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1327c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': texts[:,:-1],\n",
    "    },\n",
    "    {\n",
    "        'outputs': texts[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222cc439",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36c828c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"\n",
    "    query: (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    key: (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    value: (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    if mask is not None:\n",
    "        logits += mask * -1e9  # FYI, 0은 softmax에서 양수값을 가진다.\n",
    "\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        # d_model을 num_heads로 나눈 값.\n",
    "        # 논문 기준 : 64 (512 // 8)\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        # WO에 해당하는 밀집층 정의\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(\n",
    "            inputs, perm=[0, 2, 1, 3]\n",
    "        )  # (batch, heads, max 문장 토큰 갯수, 64)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = (\n",
    "            inputs[\"query\"],\n",
    "            inputs[\"key\"],\n",
    "            inputs[\"value\"],\n",
    "            inputs[\"mask\"],\n",
    "        )\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "        # q : (batch_size, query의 문장 길이, d_model)\n",
    "        # k : (batch_size, key의 문장 길이, d_model)\n",
    "        # v : (batch_size, value의 문장 길이, d_model)\n",
    "        # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 2. 헤드 나누기\n",
    "        # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "        # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "        # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "        # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 4. 헤드 연결(concatenate)하기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 5. WO에 해당하는 밀집층 지나기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b67dd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    # x (batch_size, max 문장 토큰 수)\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "# 가릴곳: 1, 참조할곳: 0\n",
    "def create_look_ahead_mask(x):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "# https://github.com/tensorflow/models/blob/952b8f05f42aaf27b083d0cd53946a2a2e9a4c69/official/nlp/modeling/layers/position_embedding.py#L27\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length,\n",
    "        initializer=\"he_normal\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.initializer = tf.keras.initializers.get(initializer)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"initializer\": tf.keras.initializers.serialize(self.initializer),\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        feature_size = inputs_shape[-1]\n",
    "        self.position_embeddings = self.add_weight(\n",
    "            name=\"embeddings\",\n",
    "            shape=[self.sequence_length, feature_size],\n",
    "            initializer=self.initializer,\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, start_index=0):\n",
    "        shape = tf.shape(inputs)\n",
    "        feature_length = shape[-1]\n",
    "        sequence_length = shape[-2]\n",
    "        position_embeddings = tf.convert_to_tensor(self.position_embeddings)\n",
    "        position_embeddings = tf.slice(\n",
    "            position_embeddings,\n",
    "            (start_index, 0),\n",
    "            (sequence_length, feature_length),\n",
    "        )\n",
    "        return tf.broadcast_to(position_embeddings, shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape    \n",
    "    \n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(d_model, num_heads, name=\"attention_1\")(\n",
    "        inputs={\n",
    "            \"query\": inputs,\n",
    "            \"key\": inputs,\n",
    "            \"value\": inputs,\n",
    "            \"mask\": look_ahead_mask,\n",
    "        }\n",
    "    )\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation=\"relu\")(attention1)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention1)\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "\n",
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, seq_length, name=\"decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 임베딩\n",
    "    embeddings = PositionalEmbedding(seq_length)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"decoder_layer_{}\".format(i),\n",
    "        )(inputs=[outputs, look_ahead_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a36f4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(\n",
    "    vocab_size, num_layers, units, d_model, num_heads, dropout, seq_length, name=\"transformer\",\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None), name=\"look_ahead_mask\"\n",
    "    )(inputs)\n",
    "\n",
    "    # 디코더\n",
    "    outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        seq_length=seq_length,\n",
    "    )(inputs=[inputs, look_ahead_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[\n",
    "            inputs,\n",
    "        ],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8c5c8dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    12936704    inputs[0][0]                     \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8819)   4524147     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,460,851\n",
      "Trainable params: 17,460,851\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 12  # GPT의 경우 인코더 층의 개수로 설정됨\n",
    "D_MODEL = 768  # GPT 모델에서 사용된 모델 크기\n",
    "NUM_HEADS = 12  # GPT에서 사용된 멀티 헤드 어텐션의 헤드 수\n",
    "UNITS = 3072  # GPT에서 사용된 피드 포워드 신경망의 은닉층 크기\n",
    "DROPOUT = 0.1  # GPT에서 사용된 드롭아웃 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    seq_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f1e3de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 19s 52ms/step - loss: 2.6307 - accuracy: 0.0469\n",
      "Epoch 2/20\n",
      "299/299 [==============================] - 15s 51ms/step - loss: 2.2900 - accuracy: 0.0544\n",
      "Epoch 3/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2587 - accuracy: 0.0544\n",
      "Epoch 4/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2496 - accuracy: 0.0544\n",
      "Epoch 5/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2454 - accuracy: 0.0544\n",
      "Epoch 6/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2418 - accuracy: 0.0544\n",
      "Epoch 7/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2399 - accuracy: 0.0544\n",
      "Epoch 8/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2365 - accuracy: 0.0544\n",
      "Epoch 9/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2337 - accuracy: 0.0544\n",
      "Epoch 10/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2354 - accuracy: 0.0545\n",
      "Epoch 11/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2386 - accuracy: 0.0544\n",
      "Epoch 12/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2393 - accuracy: 0.0544\n",
      "Epoch 13/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2370 - accuracy: 0.0544\n",
      "Epoch 14/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2314 - accuracy: 0.0545\n",
      "Epoch 15/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2271 - accuracy: 0.0545\n",
      "Epoch 16/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2239 - accuracy: 0.0545\n",
      "Epoch 17/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2295 - accuracy: 0.0545\n",
      "Epoch 18/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2301 - accuracy: 0.0546\n",
      "Epoch 19/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2327 - accuracy: 0.0545\n",
      "Epoch 20/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2277 - accuracy: 0.0545\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=loss_function, \n",
    "    metrics=[accuracy],\n",
    ")\n",
    "\n",
    "history = model.fit(dataset, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c338405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess(sentence)\n",
    "    sentence = tf.expand_dims(\n",
    "        SOS + tokenizer.encode(sentence), axis=0\n",
    "    )\n",
    "    output_sequence = tf.expand_dims(SOS, 0)\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if tf.equal(predicted_id, EOS[0]):\n",
    "            break\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "    return tf.squeeze(output_sequence, axis=0)\n",
    "\n",
    "\n",
    "def sentence_generation(sentence):    \n",
    "    prediction = decoder_inference(sentence)\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size]\n",
    "    )\n",
    "    print(f\"🧑 : {sentence}\")\n",
    "    print(f\"🤖 : {predicted_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3bc38a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑 : 안녕하세요!\n",
      "🤖 : \n"
     ]
    }
   ],
   "source": [
    "sentence_generation('안녕하세요!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8f39f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑 : 실패!!!!\n",
      "🤖 : \n"
     ]
    }
   ],
   "source": [
    "sentence_generation('실패!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1093ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
